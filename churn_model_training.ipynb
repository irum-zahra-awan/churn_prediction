{"cells":[{"cell_type":"code","source":["dbutils.library.installPyPI('h2o-pysparkling-2.4')\nfrom pysparkling import * # Import PySparkling\nimport h2o\nimport pandas as pd\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\nfrom pyspark.sql.functions import *\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom pysparkling.ml import H2OAutoML\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import SQLTransformer\nfrom pyspark.sql.types import *\nfrom datetime import datetime,date "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"967dbf3d-1f8a-4ac0-849d-ad215202ca03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["hc = H2OContext.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f784639-871a-4e1e-81f5-225118320314"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Connecting to H2O server at http://10.139.64.4:9009 ... successful.\n--------------------------  -------------------------------------------------------------------------------------------------------\nH2O_cluster_uptime:         19 secs\nH2O_cluster_timezone:       Etc/UTC\nH2O_data_parsing_timezone:  UTC\nH2O_cluster_version:        3.32.0.2\nH2O_cluster_version_age:    15 days\nH2O_cluster_name:           sparkling-water-root_app-20201203023605-0000\nH2O_cluster_total_nodes:    4\nH2O_cluster_free_memory:    25.29 Gb\nH2O_cluster_total_cores:    16\nH2O_cluster_allowed_cores:  16\nH2O_cluster_status:         locked, healthy\nH2O_connection_url:         http://10.139.64.4:9009\nH2O_connection_proxy:       null\nH2O_internal_security:      False\nH2O_API_Extensions:         XGBoost, Algos, Amazon S3, Sparkling Water REST API Extensions, AutoML, Core V3, TargetEncoder, Core V4\nPython_version:             3.7.3 final\n--------------------------  -------------------------------------------------------------------------------------------------------\n\nSparkling Water Context:\n * Sparkling Water Version: 3.32.0.2-1-2.4\n * H2O name: root\n * cluster size: 4\n * list of used nodes:\n  (executorId, host, port)\n  ------------------------\n  (0,10.139.64.5,54321)\n  (1,10.139.64.6,54321)\n  (2,10.139.64.7,54321)\n  (3,10.139.64.8,54321)\n  ------------------------\n\n  Open H2O Flow in browser: https://westeurope.azuredatabricks.net/driver-proxy/o/8767374559588946/1016-113926-refer971/9009/flow/index.html (CMD + click in Mac OSX)\n\n    \n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Connecting to H2O server at http://10.139.64.4:9009 ... successful.\n--------------------------  -------------------------------------------------------------------------------------------------------\nH2O_cluster_uptime:         19 secs\nH2O_cluster_timezone:       Etc/UTC\nH2O_data_parsing_timezone:  UTC\nH2O_cluster_version:        3.32.0.2\nH2O_cluster_version_age:    15 days\nH2O_cluster_name:           sparkling-water-root_app-20201203023605-0000\nH2O_cluster_total_nodes:    4\nH2O_cluster_free_memory:    25.29 Gb\nH2O_cluster_total_cores:    16\nH2O_cluster_allowed_cores:  16\nH2O_cluster_status:         locked, healthy\nH2O_connection_url:         http://10.139.64.4:9009\nH2O_connection_proxy:       null\nH2O_internal_security:      False\nH2O_API_Extensions:         XGBoost, Algos, Amazon S3, Sparkling Water REST API Extensions, AutoML, Core V3, TargetEncoder, Core V4\nPython_version:             3.7.3 final\n--------------------------  -------------------------------------------------------------------------------------------------------\n\nSparkling Water Context:\n * Sparkling Water Version: 3.32.0.2-1-2.4\n * H2O name: root\n * cluster size: 4\n * list of used nodes:\n  (executorId, host, port)\n  ------------------------\n  (0,10.139.64.5,54321)\n  (1,10.139.64.6,54321)\n  (2,10.139.64.7,54321)\n  (3,10.139.64.8,54321)\n  ------------------------\n\n  Open H2O Flow in browser: https://westeurope.azuredatabricks.net/driver-proxy/o/8767374559588946/1016-113926-refer971/9009/flow/index.html (CMD + click in Mac OSX)\n\n    \n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_data():\n  '''Read all locations' train data'''\n  all_data_df=spark.table('churn.train_data_2019_2020_all_loc')\n  return all_data_df\n\ndef filter_data(all_data_df, location_id):\n  '''Filter data by location id and prepare the dataframe for training'''\n\n  train_df=all_data_df[all_data_df.location_id==location_id]\n  train_df=train_df.drop('user_id','location_id','location_name','updated_on') # drop the columns not needed in train set\n  train_df=train_df.withColumnRenamed('num_days_before_end2019_purchased','num_days_before_end_purchased') # renaming\n  train_df=train_df.withColumn('tag_churn',train_df[\"tag_churn\"].cast(StringType())) # type coversion\n  return train_df \n\ndef get_feature_importance(leader_board_pd):\n  '''Get features importance of selected model bi model_id and plot them'''\n  \n  model_id=leader_board_pd[\"model_id\"][2]\n  m = h2o.get_model(model_id)\n  \n  print('Plot Feature Importance---')\n  #print(m.varimp_plot()) #Variable importance\n  \n  print('Feature importance Values------')\n  print(m.varimp(use_pandas=True))\n  \n  print(f'*** PLOTTING {model_id}***')\n  #print(m.plot())\n  \ndef get_metrics(leader_board_pd):\n  '''compute the metrics of the model trained by id '''\n  \n  model_id=leader_board_pd[\"model_id\"][0]\n  model = h2o.get_model(model_id)\n  \n  print(\"Model Perfomance---\")\n  print(model.model_performance())\n  print(\"--------------------\")\n  \n  threshold=model.find_threshold_by_max_metric(metric='F1')\n  precision=(model.precision(thresholds=threshold)[0][1])*100\n  recall=(model.recall(thresholds=threshold)[0][1])*100\n  accuracy=(model.accuracy(thresholds=threshold)[0][1])*100\n  f1=model.F1(thresholds=threshold)[0][1]\n  specificity=100*(model.tnr(thresholds=threshold)[0][1])\n  mcc=model.mcc(thresholds=threshold)[0][1]\n  \n  print('Saving these metrics for leader model')\n  print(f'Threshold to consider for mx f1 metric based is {threshold}')\n  print(f'The model sensitivity/recall or total positive rate ie model will catch {recall} % of customers who will actually churn')\n  print(f'The model specificity or True negative rate ie model will catch {specificity} % of customers who will actually Notchurn')\n  print(f'Precision ie Out of all customers it predicted as it will churn {precision} % of them will actually churn')      \n  print(f'Overall accuracy is {accuracy}%')\n  print(f'Max F1 score {f1}')\n  print(f'Mcc score is {mcc}')\n  \n  return threshold,precision,recall,accuracy,f1,specificity,mcc\n  \ndef save_model(model, location_name, updated_on, best_model_df):\n  '''save the trained leader model and metrics'''\n  \n  # SAVE MODEL\n  location_name = location_name.replace(\" \", \"\")\n  print('Saving Model For:' , location_name)\n  path_tomodel=\"/dbfs/FileStore/df/Churn_Models/h2o_leader_model_train_\"+location_name+\"_\"+str(updated_on)+\".model\"\n  model.write().overwrite().save(path_tomodel)\n  \n  # SAVE METRICS\n \n  best_model_df_sp = spark.createDataFrame(best_model_df)\n  best_model_df_sp.write.format('delta').mode('append').saveAsTable('churn.h2o_leader_model_train_metrics')\n  \n  \ndef train_model(train_df,location_id,location_name):\n  '''Train the model, get the leaderboard and save them'''\n  \n  InputdataTransformer = SQLTransformer(statement=\"SELECT * FROM __THIS__\")\n  automlEstimator = H2OAutoML(maxModels=26,seed=1,splitRatio=0.75,nfolds=5,balanceClasses=True,labelCol=\"tag_churn\")\n  #Building Pipeline of Inputs and Estimator\n  pipeline = Pipeline(stages=[InputdataTransformer, automlEstimator])\n  \n  # Train thr model \n  model = automlEstimator.fit(train_df)\n  \n  # Get leaderboard \n  lb = automlEstimator.getLeaderboard()\n  return model, lb\n  \ndef main_training_function():\n  '''main function retrieving data for all locations, training the models and saving them'''\n  \n  # Get all the location ids and names\n  inputs_df=spark.sql('select * from churn.all_location_inputs_train_test')\n  all_locations_ids = [x[\"location_id\"] for x in inputs_df.collect()]\n  print('Total Locations=',len(all_locations_ids))\n  \n  # Get all locations train data\n  all_data_df=read_data()\n    \n  # Train seprate models for each location in this loop\n  for loc_id in all_locations_ids:\n    if int(loc_id)>0:\n      location_id = loc_id\n      location_name = inputs_df.where(inputs_df.location_id == loc_id).select('location_name').collect()[0]['location_name']\n      print(f'Start Training Model For Location ID: {loc_id} and Location Name: {location_name}')\n\n      # Filter and prepare train data by location_id\n      train_df=filter_data(all_data_df, location_id)\n\n      # Model training and saving\n      model, leader_board = train_model(train_df,location_id,location_name)\n\n      leader_board_pd=leader_board.toPandas()\n\n      get_feature_importance(leader_board_pd)\n      threshold,precision,recall,accuracy,f1,specificity,mcc = get_metrics(leader_board_pd) \n\n      updated_on=date.today() \n      best_model_df=leader_board_pd[:1]\n      best_model_df['location_name']=location_name\n      best_model_df['updated_on']=updated_on\n      best_model_df['threshold']=threshold\n      best_model_df['precision']=precision\n      best_model_df['recall']=recall\n      best_model_df['accuracy']=accuracy\n      best_model_df['f1']=f1\n      best_model_df['specificity']=specificity\n      best_model_df['mcc']=mcc\n\n      save_model(model, location_name, updated_on,best_model_df)\n    \n  print('DONE....')\n    \n# def main_train_function_for_debugging():\n#   '''Use this function to debug/test for one hardcoded location only'''  \n#   # HARDCODE THE INPUTS HERE\n#   location_id = 9\n#   location_name = 'testrun_for_Qatar'\n#   print(f'Start Training Model For Location ID: {location_id} and Location Name: {location_name}')\n\n#   # Get all locations train data\n#   all_data_df=read_data()\n\n#   # Filter and prepare train data by location_id\n#   train_df=filter_data(all_data_df, location_id)\n\n#  # Model training and saving\n#   model, leader_board = train_model(train_df,location_id,location_name)\n  \n#   try:\n#     model_id=leader_board[\"model_id\"][2]\n#     print('model_id ',model_id)\n#     feature_imp_model = get_feature_importance(model_id)\n\n#     threshold,precision,recall,accuracy,f1,specificity,mcc = get_metrics(model) \n#     leader_board = leader_board.toPandas()\n#     updated_on=date.today() \n#     best_model_df=leader_board[:1]\n#     best_model_df['location_name']=location_name\n#     best_model_df['updated_on']=updated_on\n#     best_model_df['threshold']=threshold\n#     best_model_df['precision']=precision\n#     best_model_df['recall']=recall\n#     best_model_df['accuracy']=accuracy\n#     best_model_df['f1']=f1\n#     best_model_df['specificity']=specificity\n#     best_model_df['mcc']=mcc\n#   except Exception as e:\n#     print(e)\n#   try:    \n#     save_model(model, location_name, updated_on,best_model_df)\n#   except Exception as e:\n#     print('saving: ', e)\n  \n#   print('DONE....')\n#   return model, leader_board#best_model_df, feature_imp_model\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43e8b97c-0b69-42af-b055-3d9e94467335"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["main_training_function()\n# model, leader_board = main_train_function_for_debugging() # for testing/debugging purpose"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Main Function To Train Model For All Locations","showTitle":true,"inputWidgets":{},"nuid":"af721896-f7e2-4711-97f2-af473241bc57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Total Locations= 13\nStart Training Model For Location ID: 1 and Location Name: Dubai\nPlot Feature Importance---\nFeature importance Values------\n                                     variable  ...  percentage\n0                    diff_num_months_1st_last  ...    0.244990\n1      num_days_before_endyear_active_session  ...    0.196151\n2                   num_of_months_redemptions  ...    0.067098\n3               num_days_before_end_purchased  ...    0.064571\n4                  num_distinct_months_active  ...    0.063033\n5   num_days_before_endyear_active_redemption  ...    0.039057\n6                    num_of_weeks_redemptions  ...    0.035665\n7                             tag_nationality  ...    0.034555\n8                                tenure_inapp  ...    0.028962\n9                                 total_spend  ...    0.018037\n10                           avg_recency_days  ...    0.015211\n11                     total_paid_for_product  ...    0.015043\n12                                  age_group  ...    0.013491\n13                                    cor_tag  ...    0.012851\n14                     total_sessions_monthly  ...    0.012454\n15                                       home  ...    0.009602\n16                         median_spend_redem  ...    0.007731\n17                     total_owned_products.1  ...    0.007392\n18                       median_monthly_spend  ...    0.006925\n19                                quicksearch  ...    0.006080\n20            median_num_days_permonth_active  ...    0.005622\n21            total_monthly_spend_redemptions  ...    0.005599\n22                num_of_merchants_interacted  ...    0.005289\n23                   num_distinct_days_active  ...    0.005238\n24                avg_monthly_sessions_active  ...    0.005126\n25                     total_owned_products.0  ...    0.004981\n26                     avg_recency_days_inapp  ...    0.003992\n27                            count_num_areas  ...    0.003909\n28                             merchantdetail  ...    0.003654\n29                             total_sessions  ...    0.003654\n30                             redemptioncard  ...    0.003653\n31                            min_spend_redem  ...    0.003586\n32                                 offerslist  ...    0.003549\n33                 count_of_total_redemptions  ...    0.003445\n34     percentage_activity_of_redeem_appvisit  ...    0.003403\n35                    num_of_days_redemptions  ...    0.003347\n36                            max_spend_redem  ...    0.003100\n37                              notifications  ...    0.003091\n38                              advancesearch  ...    0.002984\n39                               categoryhome  ...    0.002853\n40           num_distinct_merchant_categories  ...    0.002506\n41                                     gender  ...    0.002328\n42     percentage_of_merchants_redeem_present  ...    0.002143\n43                     total_owned_products.2  ...    0.002117\n44                  num_distinct_merchant_ids  ...    0.002112\n45                 avg_weekly_sessions_active  ...    0.001861\n46                           has_redem_buffet  ...    0.001606\n47                                    product  ...    0.001590\n48                                     offers  ...    0.001512\n49                                 favourites  ...    0.001510\n50                                has_connect  ...    0.000666\n51                   is_savings_morethan_paid  ...    0.000422\n52                      offertime_of_purchase  ...    0.000381\n53                     total_owned_products.3  ...    0.000230\n54                              zero_redeemer  ...    0.000039\n55                     total_owned_products.4  ...    0.000000\n\n[56 rows x 4 columns]\n*** PLOTTING GBM_2_AutoML_20201203_092049***\nModel Perfomance---\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.1304220385244704\nRMSE: 0.36113991544063695\nLogLoss: 0.4147123727066082\nNull degrees of freedom: 9920\nResidual degrees of freedom: 9899\nNull deviance: 13668.939940787548\nResidual deviance: 8228.72289924452\nAIC: 8272.72289924452\nAUC: 0.9023709184648403\nAUCPR: 0.8957801615299349\nGini: 0.8047418369296806\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.4408392098641903: \n       0     1     Error    Rate\n-----  ----  ----  -------  ---------------\n0      4533  885   0.1633   (885.0/5418.0)\n1      885   3618  0.1965   (885.0/4503.0)\nTotal  5418  4503  0.1784   (1770.0/9921.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.440839     0.803464  213\nmax f2                       0.231759     0.863883  309\nmax f0point5                 0.575577     0.832924  157\nmax accuracy                 0.483107     0.823506  196\nmax precision                0.961692     1         0\nmax recall                   0.108148     1         385\nmax specificity              0.961692     1         0\nmax absolute_mcc             0.483107     0.643292  196\nmax min_per_class_accuracy   0.421306     0.815012  221\nmax mean_per_class_accuracy  0.448098     0.820439  209\nmax tns                      0.961692     5418      0\nmax fns                      0.961692     4445      0\nmax fps                      0.0693138    5418      399\nmax tps                      0.108148     4503      385\nmax tnr                      0.961692     1         0\nmax fnr                      0.961692     0.98712   0\nmax fpr                      0.0693138    1         399\nmax tpr                      0.108148     1         385\n\nGains/Lift Table: Avg response rate: 45.39 %, avg score: 46.70 %\ngroup    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ---------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.0100796                   0.960045           2.2032     2.2032             1                0.961183  1                           0.961183            0.0222074       0.0222074                  120.32    120.32             0.0222074\n2        0.0200585                   0.957683           2.2032     2.2032             1                0.958803  1                           0.959999            0.0219853       0.0441928                  120.32    120.32             0.0441928\n3        0.0300373                   0.955433           2.2032     2.2032             1                0.956642  1                           0.958883            0.0219853       0.0661781                  120.32    120.32             0.0661781\n4        0.0400161                   0.95301            2.2032     2.2032             1                0.954194  1                           0.957714            0.0219853       0.0881634                  120.32    120.32             0.0881634\n5        0.0500958                   0.949906           2.2032     2.2032             1                0.951777  1                           0.956519            0.0222074       0.110371                   120.32    120.32             0.110371\n6        0.100091                    0.926861           2.18987    2.19654            0.993952         0.940433  0.996979                    0.948484            0.109483        0.219853                   118.987   119.654            0.2193\n7        0.150086                    0.877965           2.10103    2.16473            0.953629         0.905373  0.982539                    0.934123            0.105041        0.324895                   110.103   116.473            0.320096\n8        0.200081                    0.805459           1.9811     2.11884            0.899194         0.842168  0.961713                    0.911146            0.0990451       0.42394                    98.1101   111.884            0.409912\n9        0.300071                    0.639273           1.76789    2.0019             0.802419         0.719949  0.908633                    0.847435            0.176771        0.600711                   76.7889   100.19             0.550508\n10       0.40006                     0.505043           1.43919    1.86126            0.653226         0.571087  0.844797                    0.778365            0.143904        0.744615                   43.9186   86.1255            0.63092\n11       0.50005                     0.389405           0.975004   1.68404            0.44254          0.444643  0.764362                    0.711634            0.0974906       0.842105                   -2.49961  68.4041            0.626343\n12       0.60004                     0.300757           0.672953   1.51555            0.305444         0.344194  0.687888                    0.650405            0.0672885       0.909394                   -32.7047  51.5554            0.566463\n13       0.70003                     0.23892            0.45974    1.36475            0.208669         0.26923   0.619438                    0.595959            0.0459694       0.955363                   -54.026   36.4745            0.467545\n14       0.80002                     0.186972           0.302051   1.23193            0.137097         0.211582  0.559153                    0.547918            0.0302021       0.985565                   -69.7949  23.1925            0.339755\n15       0.90001                     0.144404           0.119932   1.10838            0.0544355        0.165899  0.50308                     0.505476            0.011992        0.997557                   -88.0068  10.8384            0.17862\n16       1                           0.0676945          0.0244306  1                  0.0110887        0.120365  0.453886                    0.466969            0.00244282      1                          -97.5569  0                  0\n\n\n--------------------\nSaving these metrics for leader model\nThreshold to consider for mx f1 metric based is 0.4408392098641903\nThe model sensitivity/recall or total positive rate ie model will catch 80.34643570952699 % of customers who will actually churn\nThe model specificity or True negative rate ie model will catch 83.66555924695459 % of customers who will actually Notchurn\nPrecision ie Out of all customers it predicted as it will churn 80.34643570952699 % of them will actually churn\nOverall accuracy is 82.15905654671907%\nMax F1 score 0.8034643570952699\nMcc score is 0.6401199495648158\nSaving Model For: Dubai\nStart Training Model For Location ID: 2 and Location Name: Abu Dhabi\nPlot Feature Importance---\nFeature importance Values------\n                                     variable  ...  percentage\n0                   num_of_months_redemptions  ...    0.202941\n1      num_days_before_endyear_active_session  ...    0.162166\n2                    diff_num_months_1st_last  ...    0.117644\n3   num_days_before_endyear_active_redemption  ...    0.065908\n4                    num_of_weeks_redemptions  ...    0.056234\n5                             tag_nationality  ...    0.043008\n6               num_days_before_end_purchased  ...    0.036210\n7                                tenure_inapp  ...    0.022012\n8                  num_distinct_months_active  ...    0.019069\n9                      total_paid_for_product  ...    0.017722\n10                                total_spend  ...    0.016147\n11                    num_of_days_redemptions  ...    0.014019\n12                                  age_group  ...    0.013017\n13                                       home  ...    0.012452\n14                           avg_recency_days  ...    0.010956\n15                         median_spend_redem  ...    0.010922\n16                            max_spend_redem  ...    0.010777\n17                             merchantdetail  ...    0.010082\n18                     total_sessions_monthly  ...    0.009803\n19                            count_num_areas  ...    0.009777\n20                                quicksearch  ...    0.008919\n21                       median_monthly_spend  ...    0.008801\n22            median_num_days_permonth_active  ...    0.007876\n23                avg_monthly_sessions_active  ...    0.007829\n24            total_monthly_spend_redemptions  ...    0.007603\n25                num_of_merchants_interacted  ...    0.007536\n26                   num_distinct_days_active  ...    0.006959\n27                             total_sessions  ...    0.006874\n28                                 offerslist  ...    0.006398\n29     percentage_of_merchants_redeem_present  ...    0.005935\n30                            min_spend_redem  ...    0.005203\n31                               categoryhome  ...    0.005089\n32                                     offers  ...    0.004891\n33                              notifications  ...    0.004542\n34                                     gender  ...    0.004395\n35     percentage_activity_of_redeem_appvisit  ...    0.004257\n36                 count_of_total_redemptions  ...    0.003903\n37                              advancesearch  ...    0.003885\n38                  num_distinct_merchant_ids  ...    0.003605\n39                     avg_recency_days_inapp  ...    0.003593\n40                                    cor_tag  ...    0.003360\n41                           has_redem_buffet  ...    0.003197\n42                 avg_weekly_sessions_active  ...    0.003158\n43                             redemptioncard  ...    0.002800\n44                                    product  ...    0.002028\n45           num_distinct_merchant_categories  ...    0.001793\n46                      offertime_of_purchase  ...    0.001196\n47                                 favourites  ...    0.001136\n48                                has_connect  ...    0.000829\n49                     total_owned_products.0  ...    0.000829\n50                     total_owned_products.1  ...    0.000662\n51                              zero_redeemer  ...    0.000040\n52                   is_savings_morethan_paid  ...    0.000014\n53                     total_owned_products.2  ...    0.000000\n\n[54 rows x 4 columns]\n*** PLOTTING GBM_grid__1_AutoML_20201203_103025_model_2***\nModel Perfomance---\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.1351023172674048\nRMSE: 0.36756267121050906\nLogLoss: 0.4291679552534939\nNull degrees of freedom: 10050\nResidual degrees of freedom: 10028\nNull deviance: 13919.048361813046\nResidual deviance: 8627.134236505733\nAIC: 8673.134236505733\nAUC: 0.9010055244903263\nAUCPR: 0.9029584419233154\nGini: 0.8020110489806527\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.39659542374000495: \n       0     1     Error    Rate\n-----  ----  ----  -------  ----------------\n0      4064  1153  0.221    (1153.0/5217.0)\n1      736   4098  0.1523   (736.0/4834.0)\nTotal  4800  5251  0.1879   (1889.0/10051.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.396595     0.812692  235\nmax f2                       0.271138     0.871731  299\nmax f0point5                 0.524106     0.833333  179\nmax accuracy                 0.465039     0.818128  204\nmax precision                0.943343     1         0\nmax recall                   0.123067     1         382\nmax specificity              0.943343     1         0\nmax absolute_mcc             0.465039     0.636067  204\nmax min_per_class_accuracy   0.426325     0.815219  221\nmax mean_per_class_accuracy  0.465039     0.816701  204\nmax tns                      0.943343     5217      0\nmax fns                      0.943343     4830      0\nmax fps                      0.0775788    5217      399\nmax tps                      0.123067     4834      382\nmax tnr                      0.943343     1         0\nmax fnr                      0.943343     0.999173  0\nmax fpr                      0.0775788    1         399\nmax tpr                      0.123067     1         382\n\nGains/Lift Table: Avg response rate: 48.09 %, avg score: 47.64 %\ngroup    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ---------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.0100488                   0.935184           2.07923    2.07923            1                0.938277  1                           0.938277            0.0208937       0.0208937                  107.923   107.923            0.0208937\n2        0.0200975                   0.931636           2.07923    2.07923            1                0.933444  1                           0.935861            0.0208937       0.0417873                  107.923   107.923            0.0417873\n3        0.0300468                   0.9278             2.07923    2.07923            1                0.929819  1                           0.93386             0.0206868       0.0624741                  107.923   107.923            0.0624741\n4        0.0400955                   0.924061           2.05864    2.07407            0.990099         0.925905  0.997519                    0.931866            0.0206868       0.0831609                  105.864   107.407            0.0829693\n5        0.0500448                   0.920224           2.07923    2.0751             1                0.922073  0.998012                    0.929919            0.0206868       0.103848                   107.923   107.51             0.103656\n6        0.10009                     0.898409           2.07096    2.07303            0.996024         0.910186  0.997018                    0.920053            0.103641        0.207489                   107.096   107.303            0.206914\n7        0.150035                    0.855513           2.02124    2.05579            0.972112         0.87967   0.988727                    0.90661             0.100952        0.30844                    102.124   105.579            0.305182\n8        0.20008                     0.793685           1.89735    2.01616            0.912525         0.824872  0.969667                    0.886165            0.0949524       0.403393                   89.7349   101.616            0.3917\n9        0.30007                     0.645191           1.74614    1.92618            0.839801         0.719524  0.926393                    0.830636            0.174597        0.577989                   74.614    92.6184            0.535436\n10       0.40006                     0.517571           1.42753    1.80155            0.686567         0.577211  0.866451                    0.767296            0.142739        0.720728                   42.7531   80.1552            0.617795\n11       0.50005                     0.413801           1.06755    1.65478            0.513433         0.465387  0.795862                    0.706926            0.106744        0.827472                   6.75452   65.478             0.630807\n12       0.60004                     0.335026           0.76135    1.5059             0.366169         0.373009  0.724258                    0.651282            0.0761274       0.9036                     -23.865   50.5899            0.584834\n13       0.70003                     0.267032           0.469637   1.35788            0.225871         0.299776  0.65307                     0.601074            0.046959        0.950559                   -53.0363  35.7883            0.482665\n14       0.80002                     0.211559           0.29792    1.2254             0.143284         0.239249  0.589355                    0.555852            0.029789        0.980348                   -70.208   22.5404            0.347417\n15       0.90001                     0.159029           0.163442   1.10742            0.078607         0.185203  0.532611                    0.514673            0.0163426       0.99669                    -83.6558  10.7421            0.186263\n16       1                           0.0756336          0.0331022  1                  0.0159204        0.131884  0.480947                    0.476398            0.00330989      1                          -96.6898  0                  0\n\n\n--------------------\nSaving these metrics for leader model\nThreshold to consider for mx f1 metric based is 0.39659542374000495\nThe model sensitivity/recall or total positive rate ie model will catch 84.77451386015721 % of customers who will actually churn\nThe model specificity or True negative rate ie model will catch 77.8991757715162 % of customers who will actually Notchurn\nPrecision ie Out of all customers it predicted as it will churn 78.04227766139783 % of them will actually churn\nOverall accuracy is 81.20585016416277%\nMax F1 score 0.8126921170054537\nMcc score is 0.6269131450166762\nSaving Model For: AbuDhabi\nStart Training Model For Location ID: 3 and Location Name: Bahrain\nPlot Feature Importance---\nFeature importance Values------\n                                     variable  ...  percentage\n0                    diff_num_months_1st_last  ...    0.135930\n1                   num_of_months_redemptions  ...    0.131874\n2      num_days_before_endyear_active_session  ...    0.128018\n3                                 total_spend  ...    0.056526\n4               num_days_before_end_purchased  ...    0.044374\n5   num_days_before_endyear_active_redemption  ...    0.042968\n6                             tag_nationality  ...    0.038428\n7                      total_paid_for_product  ...    0.025319\n8                                tenure_inapp  ...    0.024371\n9                 num_of_merchants_interacted  ...    0.022059\n10                                       home  ...    0.021949\n11                                  age_group  ...    0.021074\n12                   num_of_weeks_redemptions  ...    0.018433\n13                   num_distinct_days_active  ...    0.018209\n14                     total_sessions_monthly  ...    0.017287\n15                           has_redem_buffet  ...    0.015446\n16                 count_of_total_redemptions  ...    0.015223\n17                       median_monthly_spend  ...    0.014684\n18            median_num_days_permonth_active  ...    0.013484\n19                            max_spend_redem  ...    0.012718\n20                         median_spend_redem  ...    0.012335\n21                 num_distinct_months_active  ...    0.011657\n22                                    cor_tag  ...    0.011415\n23                              advancesearch  ...    0.010803\n24                avg_monthly_sessions_active  ...    0.009955\n25                             total_sessions  ...    0.009352\n26            total_monthly_spend_redemptions  ...    0.008386\n27                            count_num_areas  ...    0.008363\n28           num_distinct_merchant_categories  ...    0.007838\n29                            min_spend_redem  ...    0.007797\n30                 avg_weekly_sessions_active  ...    0.007446\n31                                 offerslist  ...    0.006926\n32                              notifications  ...    0.006875\n33                    num_of_days_redemptions  ...    0.006081\n34                                     offers  ...    0.005628\n35                           avg_recency_days  ...    0.004936\n36     percentage_of_merchants_redeem_present  ...    0.004842\n37                               categoryhome  ...    0.004834\n38                                quicksearch  ...    0.003972\n39                   is_savings_morethan_paid  ...    0.003917\n40     percentage_activity_of_redeem_appvisit  ...    0.003699\n41                     avg_recency_days_inapp  ...    0.003414\n42                             merchantdetail  ...    0.003405\n43                                    product  ...    0.003390\n44                                     gender  ...    0.003097\n45                             redemptioncard  ...    0.002864\n46                                 favourites  ...    0.002852\n47                      offertime_of_purchase  ...    0.002791\n48                  num_distinct_merchant_ids  ...    0.002754\n49                     total_owned_products.0  ...    0.000000\n50                     total_owned_products.1  ...    0.000000\n51                     total_owned_products.2  ...    0.000000\n52                                has_connect  ...    0.000000\n53                              zero_redeemer  ...    0.000000\n\n[54 rows x 4 columns]\n*** PLOTTING GBM_5_AutoML_20201203_110750***\nModel Perfomance---\n\n*** WARNING: skipped 76251 bytes of output ***\n\n45                                has_connect  ...    0.001463\n46                                 offerslist  ...    0.001306\n47                             merchantdetail  ...    0.001138\n48                   is_savings_morethan_paid  ...    0.000209\n49                                    cor_tag  ...    0.000000\n50                              zero_redeemer  ...    0.000000\n\n[51 rows x 4 columns]\n*** PLOTTING GBM_5_AutoML_20201203_195745***\nModel Perfomance---\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.12592631750102493\nRMSE: 0.3548609833456264\nLogLoss: 0.4162468495716399\nNull degrees of freedom: 7081\nResidual degrees of freedom: 7064\nNull deviance: 9757.255727113803\nResidual deviance: 5895.720377332707\nAIC: 5931.720377332707\nAUC: 0.9349170575474801\nAUCPR: 0.9421805766278463\nGini: 0.8698341150949602\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.4967147747023179: \n       0     1     Error    Rate\n-----  ----  ----  -------  --------------\n0      2660  554   0.1724   (554.0/3214.0)\n1      423   3445  0.1094   (423.0/3868.0)\nTotal  3083  3999  0.138    (977.0/7082.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.496715     0.87581   213\nmax f2                       0.385208     0.919848  267\nmax f0point5                 0.582976     0.884733  175\nmax accuracy                 0.508035     0.862468  209\nmax precision                0.913449     1         0\nmax recall                   0.223499     1         346\nmax specificity              0.913449     1         0\nmax absolute_mcc             0.508035     0.722253  209\nmax min_per_class_accuracy   0.525781     0.858432  201\nmax mean_per_class_accuracy  0.508035     0.860549  209\nmax tns                      0.913449     3214      0\nmax fns                      0.913449     3864      0\nmax fps                      0.10372      3214      399\nmax tps                      0.223499     3868      346\nmax tnr                      0.913449     1         0\nmax fnr                      0.913449     0.998966  0\nmax fpr                      0.10372      1         399\nmax tpr                      0.223499     1         346\n\nGains/Lift Table: Avg response rate: 54.62 %, avg score: 54.22 %\ngroup    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ---------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.0100254                   0.905672           1.83092    1.83092            1                0.908998  1                           0.908998            0.0183557       0.0183557                  83.092    83.092             0.0183557\n2        0.0200508                   0.900958           1.83092    1.83092            1                0.903298  1                           0.906148            0.0183557       0.0367115                  83.092    83.092             0.0367115\n3        0.0300762                   0.896699           1.83092    1.83092            1                0.898785  1                           0.903694            0.0183557       0.0550672                  83.092    83.092             0.0550672\n4        0.0401017                   0.892287           1.83092    1.83092            1                0.894447  1                           0.901382            0.0183557       0.073423                   83.092    83.092             0.073423\n5        0.0501271                   0.886298           1.83092    1.83092            1                0.889159  1                           0.898937            0.0183557       0.0917787                  83.092    83.092             0.0917787\n6        0.100113                    0.857401           1.82058    1.82576            0.99435          0.872724  0.997179                    0.885849            0.0910031       0.182782                   82.0576   82.5756            0.18216\n7        0.150099                    0.8277             1.7792     1.81025            0.971751         0.842323  0.988711                    0.871354            0.0889349       0.271717                   77.9199   81.0251            0.267983\n8        0.200085                    0.792892           1.74817    1.79474            0.954802         0.81043   0.98024                     0.856134            0.0873837       0.3591                     74.8167   79.4741            0.350388\n9        0.300056                    0.719566           1.67576    1.7551             0.915254         0.757277  0.958588                    0.823197            0.167528        0.526629                   67.5758   75.5099            0.499249\n10       0.400028                    0.637771           1.58266    1.712              0.864407         0.677795  0.935051                    0.786859            0.158221        0.68485                    58.266    71.2004            0.627601\n11       0.5                         0.552699           1.35509    1.64064            0.740113         0.594864  0.896075                    0.748471            0.135471        0.820321                   35.5088   64.0641            0.705822\n12       0.599972                    0.461012           0.972353   1.52929            0.531073         0.509023  0.835255                    0.708572            0.0972079       0.917528                   -2.76468  52.9286            0.699731\n13       0.699944                    0.372731           0.522381   1.38547            0.285311         0.415004  0.756708                    0.666643            0.0522234       0.969752                   -47.7619  38.5472            0.594518\n14       0.799915                    0.293762           0.250846   1.24367            0.137006         0.334669  0.679259                    0.625153            0.0250776       0.994829                   -74.9154  24.3668            0.42949\n15       0.899887                    0.209929           0.0517209  1.11125            0.0282486        0.254692  0.606936                    0.583997            0.00517063      1                          -94.8279  11.1251            0.220597\n16       1                           0.100782           0          1                  0                0.166392  0.546173                    0.54219             0               1                          -100      0                  0\n\n\n--------------------\nSaving these metrics for leader model\nThreshold to consider for mx f1 metric based is 0.4967147747023179\nThe model sensitivity/recall or total positive rate ie model will catch 89.06411582213029 % of customers who will actually churn\nThe model specificity or True negative rate ie model will catch 82.76291225886746 % of customers who will actually Notchurn\nPrecision ie Out of all customers it predicted as it will churn 86.14653663415854 % of them will actually churn\nOverall accuracy is 86.20446201637955%\nMax F1 score 0.8758103470191941\nMcc score is 0.7212595920606563\nSaving Model For: Jeddah\nStart Training Model For Location ID: 20 and Location Name: Durban\nPlot Feature Importance---\nFeature importance Values------\nWarning: This model doesn&#39;t have variable importances\nNone\n*** PLOTTING StackedEnsemble_AllModels_AutoML_20201203_214758***\nModel Perfomance---\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.12266562185175474\nRMSE: 0.35023652272679207\nLogLoss: 0.39542974623950583\nNull degrees of freedom: 3953\nResidual degrees of freedom: 3948\nNull deviance: 4034.123343419653\nResidual deviance: 3127.058433262012\nAIC: 3139.058433262012\nAUC: 0.83953142374195\nAUCPR: 0.9503117501268148\nGini: 0.6790628474839\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.5636506969397733: \n       0    1     Error    Rate\n-----  ---  ----  -------  --------------\n0      285  534   0.652    (534.0/819.0)\n1      100  3035  0.0319   (100.0/3135.0)\nTotal  385  3569  0.1603   (634.0/3954.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.563651     0.90543   283\nmax f2                       0.398722     0.952788  345\nmax f0point5                 0.713573     0.886685  203\nmax accuracy                 0.600883     0.841173  266\nmax precision                0.929185     1         0\nmax recall                   0.193119     1         395\nmax specificity              0.929185     1         0\nmax absolute_mcc             0.633552     0.457581  249\nmax min_per_class_accuracy   0.795665     0.744811  146\nmax mean_per_class_accuracy  0.816034     0.751911  128\nmax tns                      0.929185     819       0\nmax fns                      0.929185     3124      0\nmax fps                      0.174712     819       399\nmax tps                      0.193119     3135      395\nmax tnr                      0.929185     1         0\nmax fnr                      0.929185     0.996491  0\nmax fpr                      0.174712     1         399\nmax tpr                      0.193119     1         395\n\nGains/Lift Table: Avg response rate: 79.29 %, avg score: 78.64 %\ngroup    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  --------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.0101163                   0.927468           1.26124   1.26124            1                0.928283  1                           0.928283            0.0127592       0.0127592                  26.1244   26.1244            0.0127592\n2        0.0202327                   0.926157           1.26124   1.26124            1                0.926823  1                           0.927553            0.0127592       0.0255183                  26.1244   26.1244            0.0255183\n3        0.0300961                   0.924958           1.26124   1.26124            1                0.925622  1                           0.92692             0.0124402       0.0379585                  26.1244   26.1244            0.0379585\n4        0.0402124                   0.923444           1.26124   1.26124            1                0.924304  1                           0.926262            0.0127592       0.0507177                  26.1244   26.1244            0.0507177\n5        0.0500759                   0.921878           1.26124   1.26124            1                0.92252   1                           0.925525            0.0124402       0.0631579                  26.1244   26.1244            0.0631579\n6        0.100152                    0.915437           1.26124   1.26124            1                0.918842  1                           0.922183            0.0631579       0.126316                   26.1244   26.1244            0.126316\n7        0.149975                    0.905894           1.23564   1.25274            0.979695         0.91106   0.993255                    0.918488            0.061563        0.187879                   23.5635   25.2736            0.182995\n8        0.200051                    0.897854           1.22302   1.2453             0.969697         0.901911  0.987358                    0.914339            0.061244        0.249123                   22.3025   24.5299            0.236913\n9        0.299949                    0.879632           1.20696   1.23253            0.956962         0.888742  0.977234                    0.905813            0.120574        0.369697                   20.6963   23.2531            0.33673\n10       0.400101                    0.860975           1.18162   1.21979            0.936869         0.869985  0.96713                     0.896845            0.118341        0.488038                   18.162    21.9787            0.424546\n11       0.5                         0.839362           1.11756   1.19936            0.886076         0.850577  0.950936                    0.887601            0.111643        0.599681                   11.7558   19.9362            0.481244\n12       0.599899                    0.809976           1.03454   1.17191            0.820253         0.824953  0.929174                    0.877168            0.103349        0.70303                    3.45394   17.1915            0.497902\n13       0.700051                    0.766559           0.984153  1.14505            0.780303         0.790679  0.907876                    0.864795            0.0985646       0.801595                   -1.58475  14.5053            0.49024\n14       0.799949                    0.694949           0.906818  1.1153             0.718987         0.73494   0.884287                    0.848579            0.0905901       0.892185                   -9.31815  11.5302            0.445299\n15       0.899848                    0.568379           0.737588  1.07337            0.58481          0.636982  0.85104                     0.825088            0.0736842       0.965869                   -26.2412  7.3369             0.318739\n16       1                           0.173362           0.340791  1                  0.270202         0.438647  0.792868                    0.786385            0.0341308       1                          -65.9209  0                  0\n\n\n--------------------\nSaving these metrics for leader model\nThreshold to consider for mx f1 metric based is 0.5636506969397733\nThe model sensitivity/recall or total positive rate ie model will catch 96.81020733652312 % of customers who will actually churn\nThe model specificity or True negative rate ie model will catch 34.798534798534796 % of customers who will actually Notchurn\nPrecision ie Out of all customers it predicted as it will churn 85.0378257214906 % of them will actually churn\nOverall accuracy is 83.96560445118867%\nMax F1 score 0.9054295942720764\nMcc score is 0.4320801332778804\nSaving Model For: Durban\nStart Training Model For Location ID: 49 and Location Name: Eastern Province\nPlot Feature Importance---\nFeature importance Values------\n                                     variable  ...  percentage\n0      num_days_before_endyear_active_session  ...    0.224113\n1                    diff_num_months_1st_last  ...    0.152531\n2                    num_of_weeks_redemptions  ...    0.139834\n3   num_days_before_endyear_active_redemption  ...    0.064121\n4                   num_of_months_redemptions  ...    0.049029\n5                                 total_spend  ...    0.047248\n6               num_days_before_end_purchased  ...    0.042119\n7                                   age_group  ...    0.029460\n8                                tenure_inapp  ...    0.028410\n9                 num_of_merchants_interacted  ...    0.025455\n10                            max_spend_redem  ...    0.022245\n11                           avg_recency_days  ...    0.015554\n12                         median_spend_redem  ...    0.014743\n13                                       home  ...    0.012830\n14                            tag_nationality  ...    0.012667\n15                 count_of_total_redemptions  ...    0.011553\n16                                     gender  ...    0.011194\n17                avg_monthly_sessions_active  ...    0.010735\n18                       median_monthly_spend  ...    0.010554\n19                     total_sessions_monthly  ...    0.008940\n20            total_monthly_spend_redemptions  ...    0.008064\n21                             total_sessions  ...    0.007470\n22                            min_spend_redem  ...    0.005757\n23                    num_of_days_redemptions  ...    0.005245\n24                 avg_weekly_sessions_active  ...    0.004316\n25                   num_distinct_days_active  ...    0.004295\n26                            count_num_areas  ...    0.004237\n27           num_distinct_merchant_categories  ...    0.003648\n28                                quicksearch  ...    0.003619\n29                             redemptioncard  ...    0.003474\n30                                     offers  ...    0.002848\n31                 num_distinct_months_active  ...    0.002603\n32                             merchantdetail  ...    0.002381\n33                              notifications  ...    0.001930\n34     percentage_of_merchants_redeem_present  ...    0.001622\n35            median_num_days_permonth_active  ...    0.001123\n36                              advancesearch  ...    0.001073\n37     percentage_activity_of_redeem_appvisit  ...    0.000841\n38                                 offerslist  ...    0.000823\n39                                 favourites  ...    0.000745\n40                  num_distinct_merchant_ids  ...    0.000465\n41                               categoryhome  ...    0.000086\n42                     total_owned_products.0  ...    0.000000\n43                     total_paid_for_product  ...    0.000000\n44                                has_connect  ...    0.000000\n45                           has_redem_buffet  ...    0.000000\n46                     avg_recency_days_inapp  ...    0.000000\n47                                    product  ...    0.000000\n48                      offertime_of_purchase  ...    0.000000\n49                                    cor_tag  ...    0.000000\n50                              zero_redeemer  ...    0.000000\n51                   is_savings_morethan_paid  ...    0.000000\n\n[52 rows x 4 columns]\n*** PLOTTING GBM_grid__1_AutoML_20201203_231646_model_3***\nModel Perfomance---\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.1259106933978815\nRMSE: 0.354838968262903\nLogLoss: 0.4126227643903785\nNull degrees of freedom: 4625\nResidual degrees of freedom: 4606\nNull deviance: 6332.855973836697\nResidual deviance: 3817.5858161397814\nAIC: 3857.5858161397814\nAUC: 0.9252813048199419\nAUCPR: 0.9409612440348879\nGini: 0.8505626096398837\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.4587762363131811: \n       0     1     Error    Rate\n-----  ----  ----  -------  --------------\n0      1533  476   0.2369   (476.0/2009.0)\n1      240   2377  0.0917   (240.0/2617.0)\nTotal  1773  2853  0.1548   (716.0/4626.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.458776     0.869104  233\nmax f2                       0.3416       0.920316  288\nmax f0point5                 0.583747     0.875223  176\nmax accuracy                 0.489916     0.846087  219\nmax precision                0.919589     1         0\nmax recall                   0.164544     1         377\nmax specificity              0.919589     1         0\nmax absolute_mcc             0.489916     0.685614  219\nmax min_per_class_accuracy   0.536131     0.841215  199\nmax mean_per_class_accuracy  0.544418     0.842052  195\nmax tns                      0.919589     2009      0\nmax fns                      0.919589     2609      0\nmax fps                      0.107026     2009      399\nmax tps                      0.164544     2617      377\nmax tnr                      0.919589     1         0\nmax fnr                      0.919589     0.996943  0\nmax fpr                      0.107026     1         399\nmax tpr                      0.164544     1         377\n\nGains/Lift Table: Avg response rate: 56.57 %, avg score: 56.29 %\ngroup    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ---------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.01016                     0.914251           1.76767    1.76767            1                0.916743  1                           0.916743            0.0179595       0.0179595                  76.7673   76.7673            0.0179595\n2        0.0201038                   0.910554           1.76767    1.76767            1                0.912378  1                           0.914584            0.0175774       0.0355369                  76.7673   76.7673            0.0355369\n3        0.0300476                   0.907063           1.76767    1.76767            1                0.908787  1                           0.912665            0.0175774       0.0531143                  76.7673   76.7673            0.0531143\n4        0.0402075                   0.903266           1.76767    1.76767            1                0.905216  1                           0.910783            0.0179595       0.0710737                  76.7673   76.7673            0.0710737\n5        0.0501513                   0.899988           1.76767    1.76767            1                0.901623  1                           0.908967            0.0175774       0.0886511                  76.7673   76.7673            0.0886511\n6        0.100086                    0.880914           1.76002    1.76386            0.995671         0.890347  0.99784                     0.899677            0.0878869       0.176538                   76.0021   76.3855            0.17604\n7        0.150022                    0.857618           1.75237    1.76003            0.991342         0.869819  0.995677                    0.889739            0.0875048       0.264043                   75.2368   76.0032            0.26255\n8        0.200173                    0.831147           1.71434    1.74858            0.969828         0.844845  0.989201                    0.878491            0.0859763       0.350019                   71.4338   74.8584            0.345042\n9        0.300043                    0.759734           1.62993    1.70909            0.922078         0.798651  0.966859                    0.851916            0.162782        0.512801                   62.9932   70.909             0.489904\n10       0.40013                     0.673124           1.44315    1.64257            0.816415         0.717495  0.929227                    0.818293            0.14444         0.657241                   44.3154   64.257             0.592035\n11       0.5                         0.57737            1.3621     1.58655            0.770563         0.628554  0.897536                    0.780394            0.136034        0.793275                   36.2103   58.6549            0.675306\n12       0.600086                    0.476768           1.00028    1.48877            0.565875         0.528876  0.842219                    0.738444            0.100115        0.893389                   0.028143  48.8768            0.67537\n13       0.699957                    0.382415           0.658095   1.37025            0.372294         0.427153  0.77517                     0.694029            0.0657241       0.959113                   -34.1905  37.0247            0.596744\n14       0.800043                    0.296637           0.320701   1.23895            0.181425         0.339587  0.700892                    0.649688            0.0320978       0.991211                   -67.9299  23.8947            0.440191\n15       0.899914                    0.22095            0.0726965  1.10952            0.0411255        0.256923  0.627672                    0.6061              0.00726022      0.998472                   -92.7303  10.9519            0.226943\n16       1                           0.105723           0.0152715  1                  0.00863931       0.174177  0.565716                    0.56287             0.00152847      1                          -98.4729  0                  0\n\n\n--------------------\nSaving these metrics for leader model\nThreshold to consider for mx f1 metric based is 0.4587762363131811\nThe model sensitivity/recall or total positive rate ie model will catch 90.82919373328238 % of customers who will actually churn\nThe model specificity or True negative rate ie model will catch 76.30662020905923 % of customers who will actually Notchurn\nPrecision ie Out of all customers it predicted as it will churn 83.31580792148615 % of them will actually churn\nOverall accuracy is 84.5222654561176%\nMax F1 score 0.8691042047531993\nMcc score is 0.6844485923592931\nSaving Model For: EasternProvince\nDONE....\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Total Locations= 13\nStart Training Model For Location ID: 1 and Location Name: Dubai\nPlot Feature Importance---\nFeature importance Values------\n                                     variable  ...  percentage\n0                    diff_num_months_1st_last  ...    0.244990\n1      num_days_before_endyear_active_session  ...    0.196151\n2                   num_of_months_redemptions  ...    0.067098\n3               num_days_before_end_purchased  ...    0.064571\n4                  num_distinct_months_active  ...    0.063033\n5   num_days_before_endyear_active_redemption  ...    0.039057\n6                    num_of_weeks_redemptions  ...    0.035665\n7                             tag_nationality  ...    0.034555\n8                                tenure_inapp  ...    0.028962\n9                                 total_spend  ...    0.018037\n10                           avg_recency_days  ...    0.015211\n11                     total_paid_for_product  ...    0.015043\n12                                  age_group  ...    0.013491\n13                                    cor_tag  ...    0.012851\n14                     total_sessions_monthly  ...    0.012454\n15                                       home  ...    0.009602\n16                         median_spend_redem  ...    0.007731\n17                     total_owned_products.1  ...    0.007392\n18                       median_monthly_spend  ...    0.006925\n19                                quicksearch  ...    0.006080\n20            median_num_days_permonth_active  ...    0.005622\n21            total_monthly_spend_redemptions  ...    0.005599\n22                num_of_merchants_interacted  ...    0.005289\n23                   num_distinct_days_active  ...    0.005238\n24                avg_monthly_sessions_active  ...    0.005126\n25                     total_owned_products.0  ...    0.004981\n26                     avg_recency_days_inapp  ...    0.003992\n27                            count_num_areas  ...    0.003909\n28                             merchantdetail  ...    0.003654\n29                             total_sessions  ...    0.003654\n30                             redemptioncard  ...    0.003653\n31                            min_spend_redem  ...    0.003586\n32                                 offerslist  ...    0.003549\n33                 count_of_total_redemptions  ...    0.003445\n34     percentage_activity_of_redeem_appvisit  ...    0.003403\n35                    num_of_days_redemptions  ...    0.003347\n36                            max_spend_redem  ...    0.003100\n37                              notifications  ...    0.003091\n38                              advancesearch  ...    0.002984\n39                               categoryhome  ...    0.002853\n40           num_distinct_merchant_categories  ...    0.002506\n41                                     gender  ...    0.002328\n42     percentage_of_merchants_redeem_present  ...    0.002143\n43                     total_owned_products.2  ...    0.002117\n44                  num_distinct_merchant_ids  ...    0.002112\n45                 avg_weekly_sessions_active  ...    0.001861\n46                           has_redem_buffet  ...    0.001606\n47                                    product  ...    0.001590\n48                                     offers  ...    0.001512\n49                                 favourites  ...    0.001510\n50                                has_connect  ...    0.000666\n51                   is_savings_morethan_paid  ...    0.000422\n52                      offertime_of_purchase  ...    0.000381\n53                     total_owned_products.3  ...    0.000230\n54                              zero_redeemer  ...    0.000039\n55                     total_owned_products.4  ...    0.000000\n\n[56 rows x 4 columns]\n*** PLOTTING GBM_2_AutoML_20201203_092049***\nModel Perfomance---\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.1304220385244704\nRMSE: 0.36113991544063695\nLogLoss: 0.4147123727066082\nNull degrees of freedom: 9920\nResidual degrees of freedom: 9899\nNull deviance: 13668.939940787548\nResidual deviance: 8228.72289924452\nAIC: 8272.72289924452\nAUC: 0.9023709184648403\nAUCPR: 0.8957801615299349\nGini: 0.8047418369296806\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.4408392098641903: \n       0     1     Error    Rate\n-----  ----  ----  -------  ---------------\n0      4533  885   0.1633   (885.0/5418.0)\n1      885   3618  0.1965   (885.0/4503.0)\nTotal  5418  4503  0.1784   (1770.0/9921.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.440839     0.803464  213\nmax f2                       0.231759     0.863883  309\nmax f0point5                 0.575577     0.832924  157\nmax accuracy                 0.483107     0.823506  196\nmax precision                0.961692     1         0\nmax recall                   0.108148     1         385\nmax specificity              0.961692     1         0\nmax absolute_mcc             0.483107     0.643292  196\nmax min_per_class_accuracy   0.421306     0.815012  221\nmax mean_per_class_accuracy  0.448098     0.820439  209\nmax tns                      0.961692     5418      0\nmax fns                      0.961692     4445      0\nmax fps                      0.0693138    5418      399\nmax tps                      0.108148     4503      385\nmax tnr                      0.961692     1         0\nmax fnr                      0.961692     0.98712   0\nmax fpr                      0.0693138    1         399\nmax tpr                      0.108148     1         385\n\nGains/Lift Table: Avg response rate: 45.39 %, avg score: 46.70 %\ngroup    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ---------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.0100796                   0.960045           2.2032     2.2032             1                0.961183  1                           0.961183            0.0222074       0.0222074                  120.32    120.32             0.0222074\n2        0.0200585                   0.957683           2.2032     2.2032             1                0.958803  1                           0.959999            0.0219853       0.0441928                  120.32    120.32             0.0441928\n3        0.0300373                   0.955433           2.2032     2.2032             1                0.956642  1                           0.958883            0.0219853       0.0661781                  120.32    120.32             0.0661781\n4        0.0400161                   0.95301            2.2032     2.2032             1                0.954194  1                           0.957714            0.0219853       0.0881634                  120.32    120.32             0.0881634\n5        0.0500958                   0.949906           2.2032     2.2032             1                0.951777  1                           0.956519            0.0222074       0.110371                   120.32    120.32             0.110371\n6        0.100091                    0.926861           2.18987    2.19654            0.993952         0.940433  0.996979                    0.948484            0.109483        0.219853                   118.987   119.654            0.2193\n7        0.150086                    0.877965           2.10103    2.16473            0.953629         0.905373  0.982539                    0.934123            0.105041        0.324895                   110.103   116.473            0.320096\n8        0.200081                    0.805459           1.9811     2.11884            0.899194         0.842168  0.961713                    0.911146            0.0990451       0.42394                    98.1101   111.884            0.409912\n9        0.300071                    0.639273           1.76789    2.0019             0.802419         0.719949  0.908633                    0.847435            0.176771        0.600711                   76.7889   100.19             0.550508\n10       0.40006                     0.505043           1.43919    1.86126            0.653226         0.571087  0.844797                    0.778365            0.143904        0.744615                   43.9186   86.1255            0.63092\n11       0.50005                     0.389405           0.975004   1.68404            0.44254          0.444643  0.764362                    0.711634            0.0974906       0.842105                   -2.49961  68.4041            0.626343\n12       0.60004                     0.300757           0.672953   1.51555            0.305444         0.344194  0.687888                    0.650405            0.0672885       0.909394                   -32.7047  51.5554            0.566463\n13       0.70003                     0.23892            0.45974    1.36475            0.208669         0.26923   0.619438                    0.595959            0.0459694       0.955363                   -54.026   36.4745            0.467545\n14       0.80002                     0.186972           0.302051   1.23193            0.137097         0.211582  0.559153                    0.547918            0.0302021       0.985565                   -69.7949  23.1925            0.339755\n15       0.90001                     0.144404           0.119932   1.10838            0.0544355        0.165899  0.50308                     0.505476            0.011992        0.997557                   -88.0068  10.8384            0.17862\n16       1                           0.0676945          0.0244306  1                  0.0110887        0.120365  0.453886                    0.466969            0.00244282      1                          -97.5569  0                  0\n\n\n--------------------\nSaving these metrics for leader model\nThreshold to consider for mx f1 metric based is 0.4408392098641903\nThe model sensitivity/recall or total positive rate ie model will catch 80.34643570952699 % of customers who will actually churn\nThe model specificity or True negative rate ie model will catch 83.66555924695459 % of customers who will actually Notchurn\nPrecision ie Out of all customers it predicted as it will churn 80.34643570952699 % of them will actually churn\nOverall accuracy is 82.15905654671907%\nMax F1 score 0.8034643570952699\nMcc score is 0.6401199495648158\nSaving Model For: Dubai\nStart Training Model For Location ID: 2 and Location Name: Abu Dhabi\nPlot Feature Importance---\nFeature importance Values------\n                                     variable  ...  percentage\n0                   num_of_months_redemptions  ...    0.202941\n1      num_days_before_endyear_active_session  ...    0.162166\n2                    diff_num_months_1st_last  ...    0.117644\n3   num_days_before_endyear_active_redemption  ...    0.065908\n4                    num_of_weeks_redemptions  ...    0.056234\n5                             tag_nationality  ...    0.043008\n6               num_days_before_end_purchased  ...    0.036210\n7                                tenure_inapp  ...    0.022012\n8                  num_distinct_months_active  ...    0.019069\n9                      total_paid_for_product  ...    0.017722\n10                                total_spend  ...    0.016147\n11                    num_of_days_redemptions  ...    0.014019\n12                                  age_group  ...    0.013017\n13                                       home  ...    0.012452\n14                           avg_recency_days  ...    0.010956\n15                         median_spend_redem  ...    0.010922\n16                            max_spend_redem  ...    0.010777\n17                             merchantdetail  ...    0.010082\n18                     total_sessions_monthly  ...    0.009803\n19                            count_num_areas  ...    0.009777\n20                                quicksearch  ...    0.008919\n21                       median_monthly_spend  ...    0.008801\n22            median_num_days_permonth_active  ...    0.007876\n23                avg_monthly_sessions_active  ...    0.007829\n24            total_monthly_spend_redemptions  ...    0.007603\n25                num_of_merchants_interacted  ...    0.007536\n26                   num_distinct_days_active  ...    0.006959\n27                             total_sessions  ...    0.006874\n28                                 offerslist  ...    0.006398\n29     percentage_of_merchants_redeem_present  ...    0.005935\n30                            min_spend_redem  ...    0.005203\n31                               categoryhome  ...    0.005089\n32                                     offers  ...    0.004891\n33                              notifications  ...    0.004542\n34                                     gender  ...    0.004395\n35     percentage_activity_of_redeem_appvisit  ...    0.004257\n36                 count_of_total_redemptions  ...    0.003903\n37                              advancesearch  ...    0.003885\n38                  num_distinct_merchant_ids  ...    0.003605\n39                     avg_recency_days_inapp  ...    0.003593\n40                                    cor_tag  ...    0.003360\n41                           has_redem_buffet  ...    0.003197\n42                 avg_weekly_sessions_active  ...    0.003158\n43                             redemptioncard  ...    0.002800\n44                                    product  ...    0.002028\n45           num_distinct_merchant_categories  ...    0.001793\n46                      offertime_of_purchase  ...    0.001196\n47                                 favourites  ...    0.001136\n48                                has_connect  ...    0.000829\n49                     total_owned_products.0  ...    0.000829\n50                     total_owned_products.1  ...    0.000662\n51                              zero_redeemer  ...    0.000040\n52                   is_savings_morethan_paid  ...    0.000014\n53                     total_owned_products.2  ...    0.000000\n\n[54 rows x 4 columns]\n*** PLOTTING GBM_grid__1_AutoML_20201203_103025_model_2***\nModel Perfomance---\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.1351023172674048\nRMSE: 0.36756267121050906\nLogLoss: 0.4291679552534939\nNull degrees of freedom: 10050\nResidual degrees of freedom: 10028\nNull deviance: 13919.048361813046\nResidual deviance: 8627.134236505733\nAIC: 8673.134236505733\nAUC: 0.9010055244903263\nAUCPR: 0.9029584419233154\nGini: 0.8020110489806527\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.39659542374000495: \n       0     1     Error    Rate\n-----  ----  ----  -------  ----------------\n0      4064  1153  0.221    (1153.0/5217.0)\n1      736   4098  0.1523   (736.0/4834.0)\nTotal  4800  5251  0.1879   (1889.0/10051.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.396595     0.812692  235\nmax f2                       0.271138     0.871731  299\nmax f0point5                 0.524106     0.833333  179\nmax accuracy                 0.465039     0.818128  204\nmax precision                0.943343     1         0\nmax recall                   0.123067     1         382\nmax specificity              0.943343     1         0\nmax absolute_mcc             0.465039     0.636067  204\nmax min_per_class_accuracy   0.426325     0.815219  221\nmax mean_per_class_accuracy  0.465039     0.816701  204\nmax tns                      0.943343     5217      0\nmax fns                      0.943343     4830      0\nmax fps                      0.0775788    5217      399\nmax tps                      0.123067     4834      382\nmax tnr                      0.943343     1         0\nmax fnr                      0.943343     0.999173  0\nmax fpr                      0.0775788    1         399\nmax tpr                      0.123067     1         382\n\nGains/Lift Table: Avg response rate: 48.09 %, avg score: 47.64 %\ngroup    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ---------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.0100488                   0.935184           2.07923    2.07923            1                0.938277  1                           0.938277            0.0208937       0.0208937                  107.923   107.923            0.0208937\n2        0.0200975                   0.931636           2.07923    2.07923            1                0.933444  1                           0.935861            0.0208937       0.0417873                  107.923   107.923            0.0417873\n3        0.0300468                   0.9278             2.07923    2.07923            1                0.929819  1                           0.93386             0.0206868       0.0624741                  107.923   107.923            0.0624741\n4        0.0400955                   0.924061           2.05864    2.07407            0.990099         0.925905  0.997519                    0.931866            0.0206868       0.0831609                  105.864   107.407            0.0829693\n5        0.0500448                   0.920224           2.07923    2.0751             1                0.922073  0.998012                    0.929919            0.0206868       0.103848                   107.923   107.51             0.103656\n6        0.10009                     0.898409           2.07096    2.07303            0.996024         0.910186  0.997018                    0.920053            0.103641        0.207489                   107.096   107.303            0.206914\n7        0.150035                    0.855513           2.02124    2.05579            0.972112         0.87967   0.988727                    0.90661             0.100952        0.30844                    102.124   105.579            0.305182\n8        0.20008                     0.793685           1.89735    2.01616            0.912525         0.824872  0.969667                    0.886165            0.0949524       0.403393                   89.7349   101.616            0.3917\n9        0.30007                     0.645191           1.74614    1.92618            0.839801         0.719524  0.926393                    0.830636            0.174597        0.577989                   74.614    92.6184            0.535436\n10       0.40006                     0.517571           1.42753    1.80155            0.686567         0.577211  0.866451                    0.767296            0.142739        0.720728                   42.7531   80.1552            0.617795\n11       0.50005                     0.413801           1.06755    1.65478            0.513433         0.465387  0.795862                    0.706926            0.106744        0.827472                   6.75452   65.478             0.630807\n12       0.60004                     0.335026           0.76135    1.5059             0.366169         0.373009  0.724258                    0.651282            0.0761274       0.9036                     -23.865   50.5899            0.584834\n13       0.70003                     0.267032           0.469637   1.35788            0.225871         0.299776  0.65307                     0.601074            0.046959        0.950559                   -53.0363  35.7883            0.482665\n14       0.80002                     0.211559           0.29792    1.2254             0.143284         0.239249  0.589355                    0.555852            0.029789        0.980348                   -70.208   22.5404            0.347417\n15       0.90001                     0.159029           0.163442   1.10742            0.078607         0.185203  0.532611                    0.514673            0.0163426       0.99669                    -83.6558  10.7421            0.186263\n16       1                           0.0756336          0.0331022  1                  0.0159204        0.131884  0.480947                    0.476398            0.00330989      1                          -96.6898  0                  0\n\n\n--------------------\nSaving these metrics for leader model\nThreshold to consider for mx f1 metric based is 0.39659542374000495\nThe model sensitivity/recall or total positive rate ie model will catch 84.77451386015721 % of customers who will actually churn\nThe model specificity or True negative rate ie model will catch 77.8991757715162 % of customers who will actually Notchurn\nPrecision ie Out of all customers it predicted as it will churn 78.04227766139783 % of them will actually churn\nOverall accuracy is 81.20585016416277%\nMax F1 score 0.8126921170054537\nMcc score is 0.6269131450166762\nSaving Model For: AbuDhabi\nStart Training Model For Location ID: 3 and Location Name: Bahrain\nPlot Feature Importance---\nFeature importance Values------\n                                     variable  ...  percentage\n0                    diff_num_months_1st_last  ...    0.135930\n1                   num_of_months_redemptions  ...    0.131874\n2      num_days_before_endyear_active_session  ...    0.128018\n3                                 total_spend  ...    0.056526\n4               num_days_before_end_purchased  ...    0.044374\n5   num_days_before_endyear_active_redemption  ...    0.042968\n6                             tag_nationality  ...    0.038428\n7                      total_paid_for_product  ...    0.025319\n8                                tenure_inapp  ...    0.024371\n9                 num_of_merchants_interacted  ...    0.022059\n10                                       home  ...    0.021949\n11                                  age_group  ...    0.021074\n12                   num_of_weeks_redemptions  ...    0.018433\n13                   num_distinct_days_active  ...    0.018209\n14                     total_sessions_monthly  ...    0.017287\n15                           has_redem_buffet  ...    0.015446\n16                 count_of_total_redemptions  ...    0.015223\n17                       median_monthly_spend  ...    0.014684\n18            median_num_days_permonth_active  ...    0.013484\n19                            max_spend_redem  ...    0.012718\n20                         median_spend_redem  ...    0.012335\n21                 num_distinct_months_active  ...    0.011657\n22                                    cor_tag  ...    0.011415\n23                              advancesearch  ...    0.010803\n24                avg_monthly_sessions_active  ...    0.009955\n25                             total_sessions  ...    0.009352\n26            total_monthly_spend_redemptions  ...    0.008386\n27                            count_num_areas  ...    0.008363\n28           num_distinct_merchant_categories  ...    0.007838\n29                            min_spend_redem  ...    0.007797\n30                 avg_weekly_sessions_active  ...    0.007446\n31                                 offerslist  ...    0.006926\n32                              notifications  ...    0.006875\n33                    num_of_days_redemptions  ...    0.006081\n34                                     offers  ...    0.005628\n35                           avg_recency_days  ...    0.004936\n36     percentage_of_merchants_redeem_present  ...    0.004842\n37                               categoryhome  ...    0.004834\n38                                quicksearch  ...    0.003972\n39                   is_savings_morethan_paid  ...    0.003917\n40     percentage_activity_of_redeem_appvisit  ...    0.003699\n41                     avg_recency_days_inapp  ...    0.003414\n42                             merchantdetail  ...    0.003405\n43                                    product  ...    0.003390\n44                                     gender  ...    0.003097\n45                             redemptioncard  ...    0.002864\n46                                 favourites  ...    0.002852\n47                      offertime_of_purchase  ...    0.002791\n48                  num_distinct_merchant_ids  ...    0.002754\n49                     total_owned_products.0  ...    0.000000\n50                     total_owned_products.1  ...    0.000000\n51                     total_owned_products.2  ...    0.000000\n52                                has_connect  ...    0.000000\n53                              zero_redeemer  ...    0.000000\n\n[54 rows x 4 columns]\n*** PLOTTING GBM_5_AutoML_20201203_110750***\nModel Perfomance---\n\n*** WARNING: skipped 76251 bytes of output ***\n\n45                                has_connect  ...    0.001463\n46                                 offerslist  ...    0.001306\n47                             merchantdetail  ...    0.001138\n48                   is_savings_morethan_paid  ...    0.000209\n49                                    cor_tag  ...    0.000000\n50                              zero_redeemer  ...    0.000000\n\n[51 rows x 4 columns]\n*** PLOTTING GBM_5_AutoML_20201203_195745***\nModel Perfomance---\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.12592631750102493\nRMSE: 0.3548609833456264\nLogLoss: 0.4162468495716399\nNull degrees of freedom: 7081\nResidual degrees of freedom: 7064\nNull deviance: 9757.255727113803\nResidual deviance: 5895.720377332707\nAIC: 5931.720377332707\nAUC: 0.9349170575474801\nAUCPR: 0.9421805766278463\nGini: 0.8698341150949602\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.4967147747023179: \n       0     1     Error    Rate\n-----  ----  ----  -------  --------------\n0      2660  554   0.1724   (554.0/3214.0)\n1      423   3445  0.1094   (423.0/3868.0)\nTotal  3083  3999  0.138    (977.0/7082.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.496715     0.87581   213\nmax f2                       0.385208     0.919848  267\nmax f0point5                 0.582976     0.884733  175\nmax accuracy                 0.508035     0.862468  209\nmax precision                0.913449     1         0\nmax recall                   0.223499     1         346\nmax specificity              0.913449     1         0\nmax absolute_mcc             0.508035     0.722253  209\nmax min_per_class_accuracy   0.525781     0.858432  201\nmax mean_per_class_accuracy  0.508035     0.860549  209\nmax tns                      0.913449     3214      0\nmax fns                      0.913449     3864      0\nmax fps                      0.10372      3214      399\nmax tps                      0.223499     3868      346\nmax tnr                      0.913449     1         0\nmax fnr                      0.913449     0.998966  0\nmax fpr                      0.10372      1         399\nmax tpr                      0.223499     1         346\n\nGains/Lift Table: Avg response rate: 54.62 %, avg score: 54.22 %\ngroup    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ---------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.0100254                   0.905672           1.83092    1.83092            1                0.908998  1                           0.908998            0.0183557       0.0183557                  83.092    83.092             0.0183557\n2        0.0200508                   0.900958           1.83092    1.83092            1                0.903298  1                           0.906148            0.0183557       0.0367115                  83.092    83.092             0.0367115\n3        0.0300762                   0.896699           1.83092    1.83092            1                0.898785  1                           0.903694            0.0183557       0.0550672                  83.092    83.092             0.0550672\n4        0.0401017                   0.892287           1.83092    1.83092            1                0.894447  1                           0.901382            0.0183557       0.073423                   83.092    83.092             0.073423\n5        0.0501271                   0.886298           1.83092    1.83092            1                0.889159  1                           0.898937            0.0183557       0.0917787                  83.092    83.092             0.0917787\n6        0.100113                    0.857401           1.82058    1.82576            0.99435          0.872724  0.997179                    0.885849            0.0910031       0.182782                   82.0576   82.5756            0.18216\n7        0.150099                    0.8277             1.7792     1.81025            0.971751         0.842323  0.988711                    0.871354            0.0889349       0.271717                   77.9199   81.0251            0.267983\n8        0.200085                    0.792892           1.74817    1.79474            0.954802         0.81043   0.98024                     0.856134            0.0873837       0.3591                     74.8167   79.4741            0.350388\n9        0.300056                    0.719566           1.67576    1.7551             0.915254         0.757277  0.958588                    0.823197            0.167528        0.526629                   67.5758   75.5099            0.499249\n10       0.400028                    0.637771           1.58266    1.712              0.864407         0.677795  0.935051                    0.786859            0.158221        0.68485                    58.266    71.2004            0.627601\n11       0.5                         0.552699           1.35509    1.64064            0.740113         0.594864  0.896075                    0.748471            0.135471        0.820321                   35.5088   64.0641            0.705822\n12       0.599972                    0.461012           0.972353   1.52929            0.531073         0.509023  0.835255                    0.708572            0.0972079       0.917528                   -2.76468  52.9286            0.699731\n13       0.699944                    0.372731           0.522381   1.38547            0.285311         0.415004  0.756708                    0.666643            0.0522234       0.969752                   -47.7619  38.5472            0.594518\n14       0.799915                    0.293762           0.250846   1.24367            0.137006         0.334669  0.679259                    0.625153            0.0250776       0.994829                   -74.9154  24.3668            0.42949\n15       0.899887                    0.209929           0.0517209  1.11125            0.0282486        0.254692  0.606936                    0.583997            0.00517063      1                          -94.8279  11.1251            0.220597\n16       1                           0.100782           0          1                  0                0.166392  0.546173                    0.54219             0               1                          -100      0                  0\n\n\n--------------------\nSaving these metrics for leader model\nThreshold to consider for mx f1 metric based is 0.4967147747023179\nThe model sensitivity/recall or total positive rate ie model will catch 89.06411582213029 % of customers who will actually churn\nThe model specificity or True negative rate ie model will catch 82.76291225886746 % of customers who will actually Notchurn\nPrecision ie Out of all customers it predicted as it will churn 86.14653663415854 % of them will actually churn\nOverall accuracy is 86.20446201637955%\nMax F1 score 0.8758103470191941\nMcc score is 0.7212595920606563\nSaving Model For: Jeddah\nStart Training Model For Location ID: 20 and Location Name: Durban\nPlot Feature Importance---\nFeature importance Values------\nWarning: This model doesn&#39;t have variable importances\nNone\n*** PLOTTING StackedEnsemble_AllModels_AutoML_20201203_214758***\nModel Perfomance---\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.12266562185175474\nRMSE: 0.35023652272679207\nLogLoss: 0.39542974623950583\nNull degrees of freedom: 3953\nResidual degrees of freedom: 3948\nNull deviance: 4034.123343419653\nResidual deviance: 3127.058433262012\nAIC: 3139.058433262012\nAUC: 0.83953142374195\nAUCPR: 0.9503117501268148\nGini: 0.6790628474839\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.5636506969397733: \n       0    1     Error    Rate\n-----  ---  ----  -------  --------------\n0      285  534   0.652    (534.0/819.0)\n1      100  3035  0.0319   (100.0/3135.0)\nTotal  385  3569  0.1603   (634.0/3954.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.563651     0.90543   283\nmax f2                       0.398722     0.952788  345\nmax f0point5                 0.713573     0.886685  203\nmax accuracy                 0.600883     0.841173  266\nmax precision                0.929185     1         0\nmax recall                   0.193119     1         395\nmax specificity              0.929185     1         0\nmax absolute_mcc             0.633552     0.457581  249\nmax min_per_class_accuracy   0.795665     0.744811  146\nmax mean_per_class_accuracy  0.816034     0.751911  128\nmax tns                      0.929185     819       0\nmax fns                      0.929185     3124      0\nmax fps                      0.174712     819       399\nmax tps                      0.193119     3135      395\nmax tnr                      0.929185     1         0\nmax fnr                      0.929185     0.996491  0\nmax fpr                      0.174712     1         399\nmax tpr                      0.193119     1         395\n\nGains/Lift Table: Avg response rate: 79.29 %, avg score: 78.64 %\ngroup    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  --------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.0101163                   0.927468           1.26124   1.26124            1                0.928283  1                           0.928283            0.0127592       0.0127592                  26.1244   26.1244            0.0127592\n2        0.0202327                   0.926157           1.26124   1.26124            1                0.926823  1                           0.927553            0.0127592       0.0255183                  26.1244   26.1244            0.0255183\n3        0.0300961                   0.924958           1.26124   1.26124            1                0.925622  1                           0.92692             0.0124402       0.0379585                  26.1244   26.1244            0.0379585\n4        0.0402124                   0.923444           1.26124   1.26124            1                0.924304  1                           0.926262            0.0127592       0.0507177                  26.1244   26.1244            0.0507177\n5        0.0500759                   0.921878           1.26124   1.26124            1                0.92252   1                           0.925525            0.0124402       0.0631579                  26.1244   26.1244            0.0631579\n6        0.100152                    0.915437           1.26124   1.26124            1                0.918842  1                           0.922183            0.0631579       0.126316                   26.1244   26.1244            0.126316\n7        0.149975                    0.905894           1.23564   1.25274            0.979695         0.91106   0.993255                    0.918488            0.061563        0.187879                   23.5635   25.2736            0.182995\n8        0.200051                    0.897854           1.22302   1.2453             0.969697         0.901911  0.987358                    0.914339            0.061244        0.249123                   22.3025   24.5299            0.236913\n9        0.299949                    0.879632           1.20696   1.23253            0.956962         0.888742  0.977234                    0.905813            0.120574        0.369697                   20.6963   23.2531            0.33673\n10       0.400101                    0.860975           1.18162   1.21979            0.936869         0.869985  0.96713                     0.896845            0.118341        0.488038                   18.162    21.9787            0.424546\n11       0.5                         0.839362           1.11756   1.19936            0.886076         0.850577  0.950936                    0.887601            0.111643        0.599681                   11.7558   19.9362            0.481244\n12       0.599899                    0.809976           1.03454   1.17191            0.820253         0.824953  0.929174                    0.877168            0.103349        0.70303                    3.45394   17.1915            0.497902\n13       0.700051                    0.766559           0.984153  1.14505            0.780303         0.790679  0.907876                    0.864795            0.0985646       0.801595                   -1.58475  14.5053            0.49024\n14       0.799949                    0.694949           0.906818  1.1153             0.718987         0.73494   0.884287                    0.848579            0.0905901       0.892185                   -9.31815  11.5302            0.445299\n15       0.899848                    0.568379           0.737588  1.07337            0.58481          0.636982  0.85104                     0.825088            0.0736842       0.965869                   -26.2412  7.3369             0.318739\n16       1                           0.173362           0.340791  1                  0.270202         0.438647  0.792868                    0.786385            0.0341308       1                          -65.9209  0                  0\n\n\n--------------------\nSaving these metrics for leader model\nThreshold to consider for mx f1 metric based is 0.5636506969397733\nThe model sensitivity/recall or total positive rate ie model will catch 96.81020733652312 % of customers who will actually churn\nThe model specificity or True negative rate ie model will catch 34.798534798534796 % of customers who will actually Notchurn\nPrecision ie Out of all customers it predicted as it will churn 85.0378257214906 % of them will actually churn\nOverall accuracy is 83.96560445118867%\nMax F1 score 0.9054295942720764\nMcc score is 0.4320801332778804\nSaving Model For: Durban\nStart Training Model For Location ID: 49 and Location Name: Eastern Province\nPlot Feature Importance---\nFeature importance Values------\n                                     variable  ...  percentage\n0      num_days_before_endyear_active_session  ...    0.224113\n1                    diff_num_months_1st_last  ...    0.152531\n2                    num_of_weeks_redemptions  ...    0.139834\n3   num_days_before_endyear_active_redemption  ...    0.064121\n4                   num_of_months_redemptions  ...    0.049029\n5                                 total_spend  ...    0.047248\n6               num_days_before_end_purchased  ...    0.042119\n7                                   age_group  ...    0.029460\n8                                tenure_inapp  ...    0.028410\n9                 num_of_merchants_interacted  ...    0.025455\n10                            max_spend_redem  ...    0.022245\n11                           avg_recency_days  ...    0.015554\n12                         median_spend_redem  ...    0.014743\n13                                       home  ...    0.012830\n14                            tag_nationality  ...    0.012667\n15                 count_of_total_redemptions  ...    0.011553\n16                                     gender  ...    0.011194\n17                avg_monthly_sessions_active  ...    0.010735\n18                       median_monthly_spend  ...    0.010554\n19                     total_sessions_monthly  ...    0.008940\n20            total_monthly_spend_redemptions  ...    0.008064\n21                             total_sessions  ...    0.007470\n22                            min_spend_redem  ...    0.005757\n23                    num_of_days_redemptions  ...    0.005245\n24                 avg_weekly_sessions_active  ...    0.004316\n25                   num_distinct_days_active  ...    0.004295\n26                            count_num_areas  ...    0.004237\n27           num_distinct_merchant_categories  ...    0.003648\n28                                quicksearch  ...    0.003619\n29                             redemptioncard  ...    0.003474\n30                                     offers  ...    0.002848\n31                 num_distinct_months_active  ...    0.002603\n32                             merchantdetail  ...    0.002381\n33                              notifications  ...    0.001930\n34     percentage_of_merchants_redeem_present  ...    0.001622\n35            median_num_days_permonth_active  ...    0.001123\n36                              advancesearch  ...    0.001073\n37     percentage_activity_of_redeem_appvisit  ...    0.000841\n38                                 offerslist  ...    0.000823\n39                                 favourites  ...    0.000745\n40                  num_distinct_merchant_ids  ...    0.000465\n41                               categoryhome  ...    0.000086\n42                     total_owned_products.0  ...    0.000000\n43                     total_paid_for_product  ...    0.000000\n44                                has_connect  ...    0.000000\n45                           has_redem_buffet  ...    0.000000\n46                     avg_recency_days_inapp  ...    0.000000\n47                                    product  ...    0.000000\n48                      offertime_of_purchase  ...    0.000000\n49                                    cor_tag  ...    0.000000\n50                              zero_redeemer  ...    0.000000\n51                   is_savings_morethan_paid  ...    0.000000\n\n[52 rows x 4 columns]\n*** PLOTTING GBM_grid__1_AutoML_20201203_231646_model_3***\nModel Perfomance---\n\nModelMetricsBinomialGLM: stackedensemble\n** Reported on train data. **\n\nMSE: 0.1259106933978815\nRMSE: 0.354838968262903\nLogLoss: 0.4126227643903785\nNull degrees of freedom: 4625\nResidual degrees of freedom: 4606\nNull deviance: 6332.855973836697\nResidual deviance: 3817.5858161397814\nAIC: 3857.5858161397814\nAUC: 0.9252813048199419\nAUCPR: 0.9409612440348879\nGini: 0.8505626096398837\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.4587762363131811: \n       0     1     Error    Rate\n-----  ----  ----  -------  --------------\n0      1533  476   0.2369   (476.0/2009.0)\n1      240   2377  0.0917   (240.0/2617.0)\nTotal  1773  2853  0.1548   (716.0/4626.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.458776     0.869104  233\nmax f2                       0.3416       0.920316  288\nmax f0point5                 0.583747     0.875223  176\nmax accuracy                 0.489916     0.846087  219\nmax precision                0.919589     1         0\nmax recall                   0.164544     1         377\nmax specificity              0.919589     1         0\nmax absolute_mcc             0.489916     0.685614  219\nmax min_per_class_accuracy   0.536131     0.841215  199\nmax mean_per_class_accuracy  0.544418     0.842052  195\nmax tns                      0.919589     2009      0\nmax fns                      0.919589     2609      0\nmax fps                      0.107026     2009      399\nmax tps                      0.164544     2617      377\nmax tnr                      0.919589     1         0\nmax fnr                      0.919589     0.996943  0\nmax fpr                      0.107026     1         399\nmax tpr                      0.164544     1         377\n\nGains/Lift Table: Avg response rate: 56.57 %, avg score: 56.29 %\ngroup    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ---------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.01016                     0.914251           1.76767    1.76767            1                0.916743  1                           0.916743            0.0179595       0.0179595                  76.7673   76.7673            0.0179595\n2        0.0201038                   0.910554           1.76767    1.76767            1                0.912378  1                           0.914584            0.0175774       0.0355369                  76.7673   76.7673            0.0355369\n3        0.0300476                   0.907063           1.76767    1.76767            1                0.908787  1                           0.912665            0.0175774       0.0531143                  76.7673   76.7673            0.0531143\n4        0.0402075                   0.903266           1.76767    1.76767            1                0.905216  1                           0.910783            0.0179595       0.0710737                  76.7673   76.7673            0.0710737\n5        0.0501513                   0.899988           1.76767    1.76767            1                0.901623  1                           0.908967            0.0175774       0.0886511                  76.7673   76.7673            0.0886511\n6        0.100086                    0.880914           1.76002    1.76386            0.995671         0.890347  0.99784                     0.899677            0.0878869       0.176538                   76.0021   76.3855            0.17604\n7        0.150022                    0.857618           1.75237    1.76003            0.991342         0.869819  0.995677                    0.889739            0.0875048       0.264043                   75.2368   76.0032            0.26255\n8        0.200173                    0.831147           1.71434    1.74858            0.969828         0.844845  0.989201                    0.878491            0.0859763       0.350019                   71.4338   74.8584            0.345042\n9        0.300043                    0.759734           1.62993    1.70909            0.922078         0.798651  0.966859                    0.851916            0.162782        0.512801                   62.9932   70.909             0.489904\n10       0.40013                     0.673124           1.44315    1.64257            0.816415         0.717495  0.929227                    0.818293            0.14444         0.657241                   44.3154   64.257             0.592035\n11       0.5                         0.57737            1.3621     1.58655            0.770563         0.628554  0.897536                    0.780394            0.136034        0.793275                   36.2103   58.6549            0.675306\n12       0.600086                    0.476768           1.00028    1.48877            0.565875         0.528876  0.842219                    0.738444            0.100115        0.893389                   0.028143  48.8768            0.67537\n13       0.699957                    0.382415           0.658095   1.37025            0.372294         0.427153  0.77517                     0.694029            0.0657241       0.959113                   -34.1905  37.0247            0.596744\n14       0.800043                    0.296637           0.320701   1.23895            0.181425         0.339587  0.700892                    0.649688            0.0320978       0.991211                   -67.9299  23.8947            0.440191\n15       0.899914                    0.22095            0.0726965  1.10952            0.0411255        0.256923  0.627672                    0.6061              0.00726022      0.998472                   -92.7303  10.9519            0.226943\n16       1                           0.105723           0.0152715  1                  0.00863931       0.174177  0.565716                    0.56287             0.00152847      1                          -98.4729  0                  0\n\n\n--------------------\nSaving these metrics for leader model\nThreshold to consider for mx f1 metric based is 0.4587762363131811\nThe model sensitivity/recall or total positive rate ie model will catch 90.82919373328238 % of customers who will actually churn\nThe model specificity or True negative rate ie model will catch 76.30662020905923 % of customers who will actually Notchurn\nPrecision ie Out of all customers it predicted as it will churn 83.31580792148615 % of them will actually churn\nOverall accuracy is 84.5222654561176%\nMax F1 score 0.8691042047531993\nMcc score is 0.6844485923592931\nSaving Model For: EasternProvince\nDONE....\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#predicted = model.transform(for_predictions)\n# updated_on=date.today()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f57201b-d7ae-4d23-8376-e87786d35842"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# %sql\n# DROP TABLE IF EXISTS churn.h2o_leader_model_train_metrics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9edfdcd-1648-40e0-ab00-540a66877cd1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fec45a7-bbde-4ba4-9c42-2856ebd42afb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4a2e9fc-5409-4f53-bc71-8153a5095cbc"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"churn_model_training","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":484938694639042}},"nbformat":4,"nbformat_minor":0}
